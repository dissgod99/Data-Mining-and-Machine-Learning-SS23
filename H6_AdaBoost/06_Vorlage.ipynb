{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "EY-Yvt_Hmbq5"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mEHFUTHoheSd"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import seaborn as sns\n",
        "from matplotlib.colors import ListedColormap\n",
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "from sklearn.model_selection import cross_validate, train_test_split\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import mean_squared_error, accuracy_score\n",
        "from tqdm import tqdm\n",
        "from sklearn import datasets\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import GradientBoostingRegressor\n",
        "\n",
        "\n",
        "sns.set(\n",
        "    context=\"notebook\",\n",
        "    style=\"whitegrid\",\n",
        "    rc={\"figure.dpi\": 120, \"scatter.edgecolors\": \"k\"},\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# Uebung 6 Boosting "
      ],
      "metadata": {
        "id": "EjG3UMGOiRHS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hilfsmethoden"
      ],
      "metadata": {
        "id": "EY-Yvt_Hmbq5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cache = {}\n",
        "\n",
        "\n",
        "def plot_decision_boundary(model, X, y):\n",
        "    \"\"\"Create a decision boundary plot that shows the predicted label for each point.\"\"\"\n",
        "    h = 0.05  # step size in the mesh\n",
        "\n",
        "    # Create color maps\n",
        "    cmap_light = ListedColormap([\"#FFAAAA\", \"#AAFFAA\", \"#AAAAFF\"])\n",
        "    cmap_bold = ListedColormap([\"#FF0000\", \"#00FF00\", \"#0000FF\"])\n",
        "\n",
        "    # Plot the decision boundary. For that, we will assign a color to each\n",
        "    # point in the mesh [x_min, x_max]x[y_min, y_max].\n",
        "    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
        "    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
        "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n",
        "\n",
        "    k = model.min_samples_leaf\n",
        "    if k in cache:\n",
        "        Z = cache[k]\n",
        "    else:\n",
        "        Z = model.predict(np.c_[xx.ravel(), yy.ravel()])\n",
        "        cache[k] = Z\n",
        "\n",
        "    # Put the result into a color plot\n",
        "    Z = Z.reshape(xx.shape)\n",
        "    plt.pcolormesh(xx, yy, Z, cmap=cmap_light)\n",
        "\n",
        "    # Plot also the training points\n",
        "    plt.scatter(X[:, 0], X[:, 1], c=y, cmap=cmap_bold, edgecolor=\"k\", s=20)\n",
        "\n",
        "    plt.xlim(xx.min(), xx.max())\n",
        "    plt.ylim(yy.min(), yy.max())\n",
        "\n",
        "\n",
        "def plot_decision_boundary_adaboost(classifier, X, y, N=1000, ax=None):\n",
        "    \"\"\"Utility function to plot decision boundary and scatter plot of data\"\"\"\n",
        "    x_min, x_max = X[:, 0].min() - 0.1, X[:, 0].max() + 0.1\n",
        "    y_min, y_max = X[:, 1].min() - 0.1, X[:, 1].max() + 0.1\n",
        "    xx, yy = np.meshgrid(np.linspace(x_min, x_max, N), np.linspace(y_min, y_max, N))\n",
        "\n",
        "    # Initialize the sample weights\n",
        "    sample_weight = np.ones(X.shape[0]) / X.shape[0]\n",
        "\n",
        "    for i, zz in enumerate(\n",
        "        classifier.staged_predict_proba(np.c_[xx.ravel(), yy.ravel()])\n",
        "    ):\n",
        "        plt.subplot(classifier.n_estimators, 2, 2 * i + 2)\n",
        "        plt.title(f\"Ensemble at Step {i + 1}\")\n",
        "        Z = zz[:, 1].reshape(xx.shape)\n",
        "        cm_bright = ListedColormap([\"#FF0000\", \"#0000FF\"])\n",
        "        # Get current axis and plot\n",
        "        ax = plt.gca()\n",
        "        ax.contourf(xx, yy, Z, 2, cmap=\"RdBu\", alpha=0.5)\n",
        "        ax.contour(xx, yy, Z, 2, cmap=\"RdBu\")\n",
        "        ax.scatter(X[:, 0], X[:, 1], c=y, cmap=cm_bright)\n",
        "        ax.set_xlabel(\"$X_0$\")\n",
        "        ax.set_ylabel(\"$X_1$\")\n",
        "        plt.xticks([])\n",
        "        plt.yticks([])\n",
        "\n",
        "        plt.subplot(classifier.n_estimators, 2, 2 * i + 1)\n",
        "        plt.title(f\"Decision Stump at Step {i + 1}\")\n",
        "        zz = classifier.estimators_[i].predict_proba(np.c_[xx.ravel(), yy.ravel()])\n",
        "        Z = zz[:, 1].reshape(xx.shape)\n",
        "\n",
        "        cm_bright = ListedColormap([\"#FF0000\", \"#0000FF\"])\n",
        "        # Get current axis and plot\n",
        "        ax = plt.gca()\n",
        "        ax.contourf(xx, yy, Z, 2, cmap=\"RdBu\", alpha=0.5)\n",
        "        ax.contour(xx, yy, Z, 2, cmap=\"RdBu\")\n",
        "        s_weights = (sample_weight / sample_weight.sum()) * 4000\n",
        "        ax.scatter(X[:, 0], X[:, 1], c=y, cmap=cm_bright, s=s_weights)\n",
        "        ax.set_xlabel(\"$X_0$\")\n",
        "        ax.set_ylabel(\"$X_1$\")\n",
        "        plt.xticks([])\n",
        "        plt.yticks([])\n",
        "\n",
        "        # Update samples weights\n",
        "        y_pred = classifier.estimators_[i].predict(X)\n",
        "        incorrect = y_pred != y\n",
        "        estimator_weight = classifier.estimator_weights_[i]\n",
        "        sample_weight *= np.exp(\n",
        "            estimator_weight\n",
        "            * incorrect\n",
        "            * ((sample_weight > 0) | (estimator_weight < 0))\n",
        "        )\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.suptitle(\n",
        "        \"AdaBoost Decision Boundaries for\\nDecision Stumps (left, scale indicates sample weight)\\nand Ensemble (right) after each Iteration\",\n",
        "        x=0.5,\n",
        "        y=1.05,\n",
        "    )\n",
        "\n",
        "# Run 10 fold cross-validation of the AdaBoost model\n",
        "def evaluate(X: np.ndarray, y, learning_rate: np.ndarray):\n",
        "    model = AdaBoostClassifier(n_estimators=300, learning_rate=learning_rate)\n",
        "    scores = cross_validate(\n",
        "        estimator=model,\n",
        "        X=X,\n",
        "        y=y,\n",
        "        scoring=\"accuracy\",\n",
        "        cv=10,\n",
        "        return_train_score=True,\n",
        "        n_jobs=2,\n",
        "    )\n",
        "    return np.mean(scores[\"train_score\"]), np.mean(scores[\"test_score\"])\n",
        "\n",
        "\n",
        "def plot_estimator_performance(accuracy_train, accuracy_test):\n",
        "    plt.figure()\n",
        "    plt.plot(accuracy_train, label=\"Train\")\n",
        "    plt.plot(accuracy_test, label=\"Test\")\n",
        "    plt.xlabel(\"Number of Estimators\")\n",
        "    plt.ylabel(\"Accuracy\")\n",
        "    plt.title(\"AdaBoost Accuracy over Iterations\")\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "\n",
        "def visualize_gradient_boosting_sine(X, y, axs, staged_y_preds, i, step):\n",
        "    # Plot sine curve fitting\n",
        "    ax = axs[i//step]\n",
        "    ax.set_xlim(0, X.max())\n",
        "    ax.set_ylim(-1, 1)\n",
        "    ax.set_xlabel(\"$x$\")\n",
        "    ax.set_ylabel(\"$sin(x) / Prediction$\")\n",
        "\n",
        "    # Obtain lines\n",
        "    error_lines = []\n",
        "    for _ in range(X.shape[0]):\n",
        "        (l,) = ax.plot([], [], c=\"red\", alpha=0.4)\n",
        "        error_lines.append(l)\n",
        "\n",
        "    (line_true,) = ax.plot(X, y, lw=2)\n",
        "    (line_pred,) = ax.plot([], [], lw=2)\n",
        "\n",
        "    # Enable legend\n",
        "    ax.legend(\n",
        "        [line_true, line_pred, error_lines[0]],\n",
        "        [\"True\", \"Predicted\", \"Residuals\"],\n",
        "        loc=\"upper right\",\n",
        "    )\n",
        "\n",
        "    # Animation function\n",
        "    print(f\"Progress: {i/len(staged_y_preds) * 100:.1f}%\", end=\"\\r\")\n",
        "\n",
        "    # Set error bars\n",
        "    for j, err_l in enumerate(error_lines):\n",
        "        err_l.set_data(\n",
        "            [X[j], X[j]], [y[j], staged_y_preds[i][j]]\n",
        "        )\n",
        "\n",
        "    # Set prediction\n",
        "    line_pred.set_data(X[:, 0], staged_y_preds[i])\n",
        "\n",
        "    mse = mean_squared_error(y, staged_y_preds[i])\n",
        "    ax.set_title(f\"Iteration: {i}, MSE: {mse:.5f}\", y=0.05)\n",
        "\n",
        "\n",
        "def plot_sine(X, y):\n",
        "    plt.figure()\n",
        "    plt.plot(X, y)\n",
        "    plt.xlabel(\"$x$\")\n",
        "    plt.ylabel(\"$sin(x)$\")\n",
        "    plt.title(\"Sinus Data\")\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def plot_loss(errors):\n",
        "    plt.figure()\n",
        "    plt.plot(errors)\n",
        "    plt.xlabel(\"Iteration\")\n",
        "    plt.ylabel(\"Least Squares Loss\")\n",
        "    plt.title(\"Gradient Boosting: Loss over Iterations\")\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def annotate_axes_lr(ax):\n",
        "    ax[0].set_title(\"Train\")\n",
        "    ax[0].set_xlabel(\"Iteration\")\n",
        "    ax[0].set_ylabel(\"MSE\")\n",
        "    ax[1].set_title(\"Test\")\n",
        "    ax[1].set_xlabel(\"Iteration\")\n",
        "    plt.legend()\n",
        "    plt.savefig(\"gradient_boosting_lr.pdf\")\n",
        "    plt.show()\n",
        "\n",
        "def plot_decision_boundary_stump(stump: DecisionTreeClassifier, X: np.ndarray, y: np.ndarray, N=1000) -> None:\n",
        "    \"\"\"Plot the decision boundary for a tree stump and scatters plot of the training data\"\"\"\n",
        "    x_min, x_max = X[:, 0].min() - 0.1, X[:, 0].max() + 0.1\n",
        "    y_min, y_max = X[:, 1].min() - 0.1, X[:, 1].max() + 0.1\n",
        "    xx, yy = np.meshgrid(np.linspace(x_min, x_max, N), np.linspace(y_min, y_max, N))\n",
        "\n",
        "    plt.figure(figsize=(4, 4))\n",
        "    plt.title(f\"Decision Boundary of a Decision Stump\")\n",
        "    zz = stump.predict_proba(np.c_[xx.ravel(), yy.ravel()])\n",
        "    Z = zz[:, 1].reshape(xx.shape)\n",
        "    cm_bright = ListedColormap([\"#FF0000\", \"#0000FF\"])\n",
        "    # Get current axis and plot\n",
        "    ax = plt.gca()\n",
        "    ax.contourf(xx, yy, Z, 2, cmap=\"RdBu\", alpha=0.5)\n",
        "    ax.contour(xx, yy, Z, 2, cmap=\"RdBu\")\n",
        "    ax.scatter(X[:, 0], X[:, 1], c=y, cmap=cm_bright)\n",
        "    ax.set_xlabel(\"$X_0$\")\n",
        "    ax.set_ylabel(\"$X_1$\")\n",
        "    plt.tight_layout()\n"
      ],
      "metadata": {
        "id": "dHLxibjymfsj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Adaboost"
      ],
      "metadata": {
        "id": "sa0CwItSWDhl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Daten Generierung"
      ],
      "metadata": {
        "id": "m1W3b9B0AASy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate data\n",
        "X, y = datasets.make_circles(noise=0.1, factor=0.4, random_state=0)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0, test_size=0.2)"
      ],
      "metadata": {
        "id": "aYEtZDqo__Mj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1a) Datensatzvisualisierung"
      ],
      "metadata": {
        "id": "j71Vx1iYl3Ap"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Beginnen wir mit einem künstlichen Datensatz, der aus zwei konzentrischen Kreisen besteht, die zwei Klassen\n",
        "repräsentieren:\n",
        "Visualisieren Sie den Datensatz in der Methode `plot_dataset`. Die Punkt der ersten und zweiten Klasse sollen dabei\n",
        "farblich unterschieden werden und Trainings- und Testpunkte verschieden markiert werden.\n"
      ],
      "metadata": {
        "id": "R176R6CeCudk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_dataset(X_train: np.ndarray, X_test: np.ndarray, y_train: np.ndarray, y_test: np.ndarray) -> None:\n",
        "    \"\"\"Creates as scatterplot for the given dataset. The points for the first class are blue and the points for the\n",
        "     second class are red. Training points are displayed as dots and testpoints are described by an X.\"\"\"\n",
        "    #FILL\n",
        "    raise NotImplementedError()"
      ],
      "metadata": {
        "id": "H7ykmPtzr3TX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot data\n",
        "plot_dataset(X_train, X_test, y_train, y_test)\n"
      ],
      "metadata": {
        "id": "g4WZ3sHYsbkb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1b) Basislerner\n",
        "\n",
        "Als Basislerner für das Boosting-Modell werden wir einen einfachen Entscheidungsbaum der Tiefe 1 (wegen seiner\n",
        "visuellen Flachheit auch Entscheidungsstumpf genannt) wählen. Um ein Gefühl für dessen Einfachheit zu bekommen,\n",
        "können wir den Stumpf auf die Daten selbst anpassen und die Entscheidungsgrenze visualisieren. Instanzieren Sie einen\n",
        "`DecisionTreeClassifier` Stumpf und trainieren und visualiserien Sie die Entscheidungsgrenze des Stumpfes\n",
        "mittels der vordefinierten Methode `plot_decision_boundary_stump`."
      ],
      "metadata": {
        "id": "I080iO67r9xj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create Model\n",
        "# FILL\n",
        "\n",
        "# Plot the decision boundary\n",
        "# FILL\n"
      ],
      "metadata": {
        "id": "OitoyXOzwfMI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Digits Datengenerierung und Visualierung"
      ],
      "metadata": {
        "id": "bSRQIr7WH86z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_digit_dataset(X_train: np.ndarray, y_train: np.ndarray) -> None:\n",
        "    \"\"\"Plots the first 5 images of the digit dataset.\"\"\"\n",
        "    plt.figure()\n",
        "    for i, (x_i, y_i) in enumerate(zip(X_train[:5], y_train[:5]), start=1):\n",
        "        plt.subplot(150 + i)\n",
        "        plt.imshow(x_i.reshape(8, 8), cmap=\"gray\")\n",
        "        plt.title(\"label = \" + str(y_i))\n",
        "        plt.axis(\"off\")\n",
        "    plt.show()\n",
        "\n",
        "# Load the digits dataset\n",
        "X, y = datasets.load_digits(return_X_y=True)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\n",
        "print(\"Digits shape:\", X.shape)\n",
        "\n",
        "# Plot samples\n",
        "plot_digit_dataset(X_train, y_train)\n"
      ],
      "metadata": {
        "id": "aibPZIhKIIXh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2a) Iterationen / Anzahl Classifier\n",
        "\n",
        "Wir werden das AdaBoost-Modell erneut mit einem Entscheidungsstumpf als Basis-Lerner für 300 Iterationen lernen.\n",
        "Für jede Iteration wird ein zusätzlicher Entscheidungsstumpf eingesetzt.\n",
        "Sklearn ermöglicht es uns über wiederholte Anwendung der Methode `staged_predict` auf einer Instanz des `AdaBoostClassifier`, stufenweise Vorhersagen zu treffen. Dies erzeugt Vorhersagen nach jeder Boosting-Iteration und erlaubt es uns, die\n",
        "Genauigkeitskurve über die Trainingsiterationen zu visualisieren.\n",
        "Fitten Sie ein `AdaBoostClassifier` über die Methode `get_acc_for_estimators` und geben Sie das Modell und\n",
        "die zugehörigen Trainings- und Testgenauigkeiten für jede Iteration zurück.\n"
      ],
      "metadata": {
        "id": "aTxr0f5RH0E_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_acc_for_estimators(X_train: np.ndarray, X_test: np.ndarray, y_train: np.ndarray, y_test: np.ndarray,\n",
        "                           n_estimators: int, learning_rate: float) -> tuple[np.ndarray, np.ndarray]:\n",
        "    \"\"\"Fits an AdaBoostClassifier on the given training data and returns the training and test accuracy for all\n",
        "     iterations.\"\"\"\n",
        "    # Create a Decision Tree with depth of 1 (stump) as weak base learner \n",
        "    # FILL\n",
        "    # Create AdaBoost model\n",
        "    # FILL\n",
        "    # Fit model\n",
        "    model.fit(X_train, y_train)\n",
        "    # Evaluate model on train/test set using sklearn accuracy_score and staged_predict\n",
        "    # FILL\n",
        "    \n",
        "    return accuracy_train, accuracy_test\n"
      ],
      "metadata": {
        "id": "DutvJhNnDXLL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "accuracy_train, accuracy_test = get_acc_for_estimators(X_train, X_test, y_train, y_test,\n",
        "                                                        n_estimators=300, learning_rate=0.01)\n",
        "plot_estimator_performance(accuracy_train, accuracy_test)"
      ],
      "metadata": {
        "id": "SVzN5wv1LbGp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lrs = [\n",
        "    # FILL\n",
        "       ]\n",
        "# Plot\n",
        "plt.figure()\n",
        "# Run with specific learning rate\n",
        "for lr in lrs:\n",
        "  # FILL\n",
        " \n",
        "  plt.plot(accuracy_train, label=f\"$lr={lr}$\")\n",
        "plt.legend()\n",
        "plt.suptitle(\"AdaBoost Learning Rate Comparison\")\n",
        "plt.xlabel(\"Number of Estimators\")\n",
        "plt.ylabel(\"Training Accuracy\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "3KtrAsbQMwZE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Gradient Boosting"
      ],
      "metadata": {
        "id": "yDs35L5kWa05"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3b) Regression\n",
        "\n",
        "Beginnen wir mit dem Lernen einer einfachen Sinuskurve.\n",
        "Mit dem Modul GradientBoostingRegressor von sklearn können wir ein GradientBoosting-Regressionsmodell\n",
        "an die erzeugte Sinuskurve anpassen. Wir werden die Residuen für insgesamt 300 Schritte anpassen und die Funktion\n",
        "der kleinsten Quadrate als Fehlerfunktion verwenden.\n",
        "Wenden Sie einen Gradient Boosting Regressor in der Funktion fit_gradient_boosting_regressor an und\n",
        "geben Sie das gelernte Model und die Trainingsfehler für jede Iteration zurück\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "-X6kGICAyZ1o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def fit_gradient_boosting_regressor(X: np.ndarray, n_samples: int, y: np.ndarray,\n",
        "                                    learning_rate: float, max_depth: int, random_state: int, loss: str):\n",
        "    \"\"\"Fits gradient tree boosting regressor and returns the learned model and training errors.\"\"\"\n",
        "    # create model and fit\n",
        "    # FILL\n",
        "    \n",
        "    # Obtain loss after each training iteration\n",
        "    # grab train_score_\n",
        "    # FILL\n",
        "    \n",
        "    return gb, train_errors"
      ],
      "metadata": {
        "id": "p59HDyvDW_lF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# data\n",
        "# Create sine function\n",
        "n_samples = 300\n",
        "X = np.linspace(0, 4 * np.pi, num=n_samples).reshape(-1, 1)\n",
        "y = np.sin(X[:, 0])\n",
        "\n",
        "# Plot sine function\n",
        "plot_sine(X, y)\n"
      ],
      "metadata": {
        "id": "pjjauejXYaNq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gb, errors = fit_gradient_boosting_regressor(X, n_samples, y, learning_rate=0.5, max_depth=1,\n",
        "                                              random_state=0, loss=\"squared_error\")\n",
        "\n",
        "# Plot against iterations\n",
        "plot_loss(errors)"
      ],
      "metadata": {
        "id": "m2_lUMTiXf59"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Obtain staged predictions, that is the predictions of the ensemble after each step\n",
        "staged_y_preds = [p for p in tqdm(gb.staged_predict(X))]\n",
        "\n",
        "# Show boosting iterations\n",
        "fig, axs = plt.subplots(nrows=3, ncols=2, figsize=(12, 15), dpi=300)\n",
        "axs = axs.flatten()\n",
        "step = 50\n",
        "for i in tqdm(range(0, n_samples, step)):\n",
        "  visualize_gradient_boosting_sine(X, y, axs, staged_y_preds, i=i, step=step)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "SrB_Z1HfXwif"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4a) Fehlerfunktionen\n",
        "Evaluieren Sie die Performanz von Gradient Boosting in der Funktion evaluate_loss_fn unter Nutzung der\n",
        "verschiedenen, oben genannten Fehlerfunktionen und geben Sie jeweils die mittleren Fehlerquadrate (MSE) für den\n",
        "Trainings- und Testdatensatz zurück "
      ],
      "metadata": {
        "id": "ZvEiBrU8fIwt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_loss_fn(X_train: np.ndarray, y_train: np.ndarray,\n",
        "                     X_test: np.ndarray, y_test: np.ndarray,\n",
        "                     loss_fn: str, learning_rate: float = 0.5, n_estimators: int = 200) -> np.ndarray:\n",
        "    \"\"\"Fits a GradientBoostingRegressor on the training data and returns the training and test mse for all iterations\"\"\"\n",
        "    # create and fit model\n",
        "    # FILL\n",
        "   \n",
        "    # Collect staged predict mean_squared_error for train/test\n",
        "    mses = []\n",
        "    # FILL\n",
        "    \n",
        "    return np.array(mses)"
      ],
      "metadata": {
        "id": "qc3yhovUf1Ni"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load regression dataset\n",
        "X, y = datasets.fetch_california_housing(return_X_y=True)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\n",
        "\n",
        "# Set up plot\n",
        "fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(10, 4))\n",
        "ax[0].set_title(\"Train MSE\")\n",
        "ax[0].set_xlabel(\"Iteration\")\n",
        "ax[0].set_ylabel(\"MSE\")\n",
        "ax[1].set_title(\"Test MSE\")\n",
        "ax[1].set_xlabel(\"Iteration\")\n",
        "\n",
        "# Run on all available loss functions\n",
        "for loss_fn in tqdm([\"squared_error\", \"absolute_error\", \"huber\", \"quantile\"]):\n",
        "    mses = evaluate_loss_fn(X_train, y_train, X_test, y_test, loss_fn)\n",
        "    ax[0].plot(mses[:, 0], label=loss_fn)\n",
        "    ax[1].plot(mses[:, 1], label=loss_fn)\n",
        "\n",
        "# Plot\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "9e076eUtZUeJ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}