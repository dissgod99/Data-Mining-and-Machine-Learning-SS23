{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "EY-Yvt_Hmbq5"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mEHFUTHoheSd"
      },
      "outputs": [],
      "source": [
        "from sklearn import datasets\n",
        "import numpy as np\n",
        "from sklearn.model_selection import cross_validate\n",
        "from sklearn.tree import DecisionTreeClassifier,DecisionTreeRegressor\n",
        "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\n",
        "from sklearn.metrics import make_scorer, mean_squared_error\n",
        "from matplotlib import pyplot as plt\n",
        "from sklearn.datasets import load_digits\n",
        "from tqdm import tqdm\n",
        "import seaborn as sns\n",
        "from rich.console import Console\n",
        "from rich.table import Table\n",
        "\n",
        "sns.set(\n",
        "    context=\"notebook\",\n",
        "    style=\"whitegrid\",\n",
        "    rc={\"figure.dpi\": 120, \"scatter.edgecolors\": \"k\"},\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# Aufgabe 5 Decision Tree vs. Random Forest "
      ],
      "metadata": {
        "id": "EjG3UMGOiRHS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hilfsmethoden"
      ],
      "metadata": {
        "id": "EY-Yvt_Hmbq5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_accuracy(arr: np.ndarray) -> float:\n",
        "  return round(arr.mean() * 100, 2)\n",
        "\n",
        "\n",
        "def compute_rmse(arr: np.ndarray) -> float:\n",
        "  return round(np.sqrt(arr.mean()), 2)\n",
        "\n",
        "def print_scores(tbl_name: str, model_scores: list[tuple[str,np.ndarray]], rmse: bool = False):\n",
        "  table = Table(title=tbl_name)\n",
        "\n",
        "  table.add_column(\"Model\", style=\"green\")\n",
        "  table.add_column(f\"Train {'RMSE' if rmse else 'Accuracy (%)'}\", style=\"cyan\", no_wrap=True)\n",
        "  table.add_column(f\"Test {'RMSE' if rmse else 'Accuracy (%)'}\", style=\"cyan\")\n",
        "\n",
        "  for name, score in model_scores:\n",
        "    if rmse:\n",
        "      table.add_row(name, str(compute_rmse(score[\"train_score\"])), str(compute_rmse(score[\"test_score\"])))\n",
        "    else:\n",
        "      table.add_row(name, str(compute_accuracy(score[\"train_score\"])), str(compute_accuracy(score[\"test_score\"])))\n",
        "\n",
        "  console = Console()\n",
        "  console.print(table)\n",
        "\n",
        "\n",
        "\n",
        "def plot_accuracy(xs: range, accuracies: np.ndarray, xlabel: str, ylabel=\"Accuracy\") -> None:\n",
        "    \"\"\"Plot results for the given accuracies.\"\"\"\n",
        "    acc_train = accuracies[:, 0]\n",
        "    acc_test = accuracies[:, 1]\n",
        "    plt.figure()\n",
        "    plt.plot(xs, acc_train, label=\"Train\", linestyle=\"--\")\n",
        "    plt.plot(xs, acc_test, label=\"Test\", linestyle=\"--\")\n",
        "    plt.xlabel(xlabel)\n",
        "    plt.ylabel(ylabel)\n",
        "    plt.xticks(xs[::5])\n",
        "    plt.legend()\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "id": "dHLxibjymfsj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1a) Decision Tree vs. Random Forest (diskret)"
      ],
      "metadata": {
        "id": "j71Vx1iYl3Ap"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Implementieren Sie die Funktion `cross_val_dt_rt`, welche eine 10-fache Kreuzvalidierung für einen\n",
        "[DecisionTreeClassifier](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html) mit Standardparametern und [RandomForestClassifier](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html) mit 20 Schätzern\n",
        "durchführt. Sie können dabei die Funktion [sklearn.model_selection.cross_validate](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.cross_validate.html) verwenden.\n",
        "Abschließend sollen die Genauigkeitsscores zurückgegeben werden.\n"
      ],
      "metadata": {
        "id": "R176R6CeCudk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def cross_val_dt_rt(load_fun) -> tuple[dict, dict]:\n",
        "    \"\"\"Conducts a 10-fold cross validation for a decision tree and random forrest and\n",
        "     returns their accuracy scores given an sklearn discrete dataset loader function.\"\"\"\n",
        "    # Load data\n",
        "    X, y = load_fun(return_X_y=True)\n",
        "    # Cross-validate Decision Tree and Random Forest\n",
        "    # FILL HERE\n",
        "\n",
        "\n",
        "\n",
        "    \n",
        "    return scores_dt, scores_rf"
      ],
      "metadata": {
        "id": "H7ykmPtzr3TX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "np.random.seed(0)\n",
        "scores_dt, scores_rf = cross_val_dt_rt(datasets.load_iris)\n",
        "\n",
        "print_scores(\"Dataset Iris\", [(\"Decision Tree\", scores_dt), (\"Random Forest\", scores_rf)])\n"
      ],
      "metadata": {
        "id": "g4WZ3sHYsbkb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1b) Decision Tree vs. Random Forest (kontinuierlich)\n",
        "\n",
        "Führen Sie eine 10-fache Kreuzvalidierung in der Funktion `cross_val_dt_rf_continuous` für kontinuierliche\n",
        "Zielvariablen durch. Es sollen hierbei erneut 20 Schätzer verwendet werden und die mittlere quadratische Abweichung\n",
        "als Metrikscore zurückgegeben werden.\\"
      ],
      "metadata": {
        "id": "I080iO67r9xj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def cross_val_dt_rf_continuous(load_fun) -> tuple[dict, dict]:\n",
        "    \"\"\"Conducts a 10-fold cross validation for a decision tree and random forrest and\n",
        "     returns their mse scores given an sklearn discrete dataset loader function.\"\"\"\n",
        "    # Load data\n",
        "    X, y = load_fun(return_X_y=True)\n",
        "    # Cross-validate Decision Tree and Random Forest\n",
        "    # FILL HERE\n",
        "\n",
        "\n",
        "\n",
        "    \n",
        "    return scores_dt, scores_rf\n"
      ],
      "metadata": {
        "id": "3g5SoaT2vie8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "np.random.seed(0)\n",
        "\n",
        "scores_dt, scores_rf = cross_val_dt_rf_continuous(datasets.load_diabetes)\n",
        "\n",
        "print_scores(\"Dataset Diabetes\", [(\"Decision Tree\", scores_dt), (\"Random Forest\", scores_rf)],rmse=True)\n"
      ],
      "metadata": {
        "id": "OitoyXOzwfMI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Run on multiple datasets\n",
        "\n",
        "# Get the data loader\n",
        "loader_classification = [\n",
        "  (\"Iris\", datasets.load_iris),\n",
        "  (\"Digits\", datasets.load_digits),\n",
        "  (\"Wine\", datasets.load_wine),\n",
        "  (\"Breast Cancer\", datasets.load_breast_cancer),\n",
        "]\n",
        "loader_regression = [\n",
        "  # (\"Boston\", datasets.load_boston),\n",
        "  (\"Diabetes\", datasets.load_diabetes),\n",
        "  (\"Linnerud\", datasets.load_linnerud),\n",
        "]\n",
        "\n",
        "# Set seed for reproducibility\n",
        "np.random.seed(0)\n",
        "\n",
        "# For each classification dataset evaluate:\n",
        "for name, load_fun in loader_classification:\n",
        "  scores_dt, scores_rf = cross_val_dt_rt(load_fun)\n",
        "\n",
        "  print_scores(f\"Dataset {name}\", [(\"Decision Tree\", scores_dt), (\"Random Forest\", scores_rf)])\n",
        "\n",
        "\n",
        "# For each regression dataset evaluate:\n",
        "for name, load_fun in loader_regression:\n",
        "  scores_dt, scores_rf = cross_val_dt_rf_continuous(load_fun)\n",
        "\n",
        "  print_scores(f\"Dataset {name}\", [(\"Decision Tree\", scores_dt), (\"Random Forest\", scores_rf)], rmse=True)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "DutvJhNnDXLL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2a) Anzahl der Bäume\n",
        "\n",
        "Der wichtigste Parameter eines Zufallswaldes ist seine Ensemblegröße, d.h. wie viele Entscheidungsbäume verwendet\n",
        "werden, um eine Mehrheitsabstimmung für die Entscheidung durchzuführen. Beginnen wir mit dem Zifferndatensatz\n",
        "und zeigen wir den Einfluss der Ensemblegröße auf die Klassifikationsgenauigkeit.\n",
        "Um den Einfluss der Anzahl der Bäume zu sehen, werden wir jeden Wert im Intervall n ∈ [1, · · · , 40] kreuzvalidieren\n",
        "und ein RandomForestClassifier-Modell mit n-Entscheidungsbäumen erstellen. Evaluieren Sie die Anzahl der\n",
        "Schätzer in Funktion evaluate_n_estimators mittels einer 10-fachen Kreuzvalidierung und geben sie die mittlere\n",
        "Trainings- und Testgenauigkeit zurück.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "-X6kGICAyZ1o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_n_estimators(X: np.ndarray, y: np.ndarray, n: int) -> tuple[float, float]:\n",
        "    \"\"\"\"Run 10 fold cross-validation of the model for a given number of trees and returns the\n",
        "    mean train and test score.\"\"\"\n",
        "    raise NotImplementedError(\"FILL IN\")\n",
        "\n",
        "\n",
        "\n",
        "# Define interval\n",
        "n_estimators = range(1, 41)\n",
        "\n",
        "# Evaluate interval\n",
        "accuracies_n_est = np.array([evaluate_n_estimators(X, y, alpha) for alpha in tqdm(n_estimators)])\n",
        "\n",
        "plot_accuracy(n_estimators, accuracies_n_est, \"Number of Trees\")"
      ],
      "metadata": {
        "id": "yXlsCx-nt3Ri"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2c) Maximale Baumtiefe\n",
        "\n",
        "Ein weiterer interessanter Aspekt, den es zu betrachten gilt, ist die Komplexität der Entscheidungsbäume. Wir können\n",
        "den Einfluss der Komplexität der Lerner auf die Leistung des Ensembles untersuchen, indem wir das obige Experiment\n",
        "wiederholen, aber statt mit einer unterschiedlichen Anzahl von Bäumen zu evaluieren, werden wir diesmal die\n",
        "maximale Baumtiefe für jeden Baum mit einer festen Anzahl von Bäumen ändern.\n",
        "Führen Sie eine 10-fache Kreuzvalidierung für gegebene Baumtiefe mit 20 Schätzern in der Funktion\n",
        "`evaluate_depth` durch.\n"
      ],
      "metadata": {
        "id": "8OSy_2ts62ac"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_depth(X: np.ndarray, y: np.ndarray, depth: int) -> tuple[float, float]:\n",
        "    \"\"\"Run 10 fold cross-validation of the model for a given tree depth and returns the\n",
        "    mean train and test score.\"\"\"\n",
        "    raise NotImplementedError(\"FILL IN\")\n",
        "\n",
        "\n",
        "X, y = load_digits(return_X_y=True)  \n",
        "# Define interval\n",
        "depths = range(1, 15)\n",
        "\n",
        "# Evaluate interval\n",
        "accuracies_depths = np.array([evaluate_depth(X, y, d) for d in tqdm(depths)])\n",
        "# Plot results\n",
        "plot_accuracy(depths, accuracies_depths, \"Tree Depth\")\n"
      ],
      "metadata": {
        "id": "_G-goRPa5-Jr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2e) Anzahl der Merkmale"
      ],
      "metadata": {
        "id": "_5sxxRD78YPp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Evaluieren Sie den den Einfluss dieses Parameters, indem Sie eine 10-fache Kreuzvalidierung mit 20 Schätzern für alle\n",
        "mögliche Anzahl von Merkmalen im Zifferndatensatz in der Funktion evaluate_features durchführen"
      ],
      "metadata": {
        "id": "ItkN_Zo_8cy1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_features(X: np.ndarray, y: np.ndarray, n_features: int) -> tuple[float, float]:\n",
        "    \"\"\"\"Run 10 fold cross-validation of the model for a given number of features per tree and returns the\n",
        "    mean train and test score.\"\"\"\n",
        "    raise NotImplementedError(\"FILL IN\")\n",
        "\n",
        "# Define interval\n",
        "n_features = range(1, X.shape[1], 1)\n",
        "\n",
        "# Evaluate interval\n",
        "accuracies_n_feat = np.array([evaluate_features(X, y, n) for n in tqdm(n_features)])\n",
        "\n",
        "# Plot results\n",
        "plot_accuracy(n_features, accuracies_n_feat, \"Max. Number of Features per Tree\")"
      ],
      "metadata": {
        "id": "2gopg_L28Ra5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "cukH5PHhXpNc"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}